apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-config
  namespace: economic-observatory
data:
  spark-defaults.conf: |
    spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
    spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
    spark.serializer=org.apache.spark.serializer.KryoSerializer
    spark.sql.adaptive.enabled=true
    spark.sql.adaptive.coalescePartitions.enabled=true
    spark.sql.adaptive.skewJoin.enabled=true
    spark.sql.streaming.checkpointLocation=/tmp/spark-checkpoints

---
apiVersion: v1
kind: Secret
metadata:
  name: spark-secrets
  namespace: economic-observatory
type: Opaque
data:
  minio-access-key: YWRtaW4=  # admin base64 encoded
  minio-secret-key: cGFzc3dvcmQxMjM=  # password123 base64 encoded

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-streaming-consumer
  namespace: economic-observatory
  labels:
    app: spark-streaming
    component: consumer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-streaming
      component: consumer
  template:
    metadata:
      labels:
        app: spark-streaming
        component: consumer
    spec:
      containers:
      - name: spark-streaming
        image: economic-observatory/spark-streaming:latest
        imagePullPolicy: IfNotPresent
        command: ["/opt/bitnami/spark/bin/spark-submit"]
        args:
        - --master
        - local[1]
        - --packages
        - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-spark_2.12:3.0.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262
        - --conf
        - "spark.hadoop.fs.s3a.endpoint=http://minio-service:9000"
        - --conf
        - "spark.hadoop.fs.s3a.access.key=$(MINIO_ACCESS_KEY)"
        - --conf
        - "spark.hadoop.fs.s3a.secret.key=$(MINIO_SECRET_KEY)"
        - --conf
        - "spark.hadoop.fs.s3a.path.style.access=true"
        - --conf
        - "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
        - --conf
        - "spark.hadoop.fs.s3a.connection.ssl.enabled=false"
        - --conf
        - "spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
        - /app/spark_streaming_consumer.py
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka-service.economic-observatory.svc.cluster.local:9092"
        - name: MINIO_ENDPOINT
          value: "minio-service.economic-observatory.svc.cluster.local:9000"
        - name: MINIO_ACCESS_KEY
          value: "admin"
        - name: MINIO_SECRET_KEY
          value: "password123"
        - name: SPARK_CONF_DIR
          value: "/opt/bitnami/spark/conf"
        - name: JAVA_HOME
          value: "/opt/bitnami/java"
        - name: SPARK_HOME
          value: "/opt/bitnami/spark"
        - name: PYSPARK_PYTHON
          value: "python3"
        - name: PYSPARK_DRIVER_PYTHON
          value: "python3"
        - name: HOME
          value: "/tmp"
        - name: SPARK_SUBMIT_OPTS
          value: "-Dspark.jars.ivy=/tmp/.ivy"
        - name: SPARK_LOCAL_IP
          value: "127.0.0.1"
        - name: SPARK_DRIVER_MEMORY
          value: "1g"
        - name: SPARK_EXECUTOR_MEMORY
          value: "1g"
        - name: SPARK_DRIVER_OPTS
          value: "-Djava.net.preferIPv4Stack=true -Dfile.encoding=UTF-8"
        volumeMounts:
        - name: spark-app
          mountPath: /app
        - name: spark-requirements
          mountPath: /app/requirements
        - name: spark-config
          mountPath: /opt/spark/conf
        - name: checkpoint-storage
          mountPath: /tmp/spark-checkpoints
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - "ps aux | grep spark_streaming_consumer.py | grep -v grep"
          initialDelaySeconds: 120
          periodSeconds: 60
          timeoutSeconds: 10
          failureThreshold: 5
      volumes:
      - name: spark-app
        configMap:
          name: spark-app-code
      - name: spark-requirements
        configMap:
          name: spark-etl-requirements
      - name: spark-config
        configMap:
          name: spark-config
      - name: checkpoint-storage
        emptyDir: {}
      restartPolicy: Always

# Note: ConfigMap will be created by setup script from external files

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-app-code
  namespace: economic-observatory

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: spark-etl-bronze-to-silver
  namespace: economic-observatory
spec:
  schedule: "0 */1 * * *"  # Run every 1 hour
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: spark-etl
            image: economic-observatory/spark-streaming:latest
            imagePullPolicy: IfNotPresent
            command: ["/opt/bitnami/spark/bin/spark-submit"]
            args:
            - --master
            - local[*]
            - --packages
            - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-spark_2.12:3.0.0
            - --conf
            - "spark.hadoop.fs.s3a.endpoint=http://minio-service:9000"
            - --conf
            - "spark.hadoop.fs.s3a.access.key=admin"
            - --conf
            - "spark.hadoop.fs.s3a.secret.key=password123"
            - --conf
            - "spark.hadoop.fs.s3a.path.style.access=true"
            - --conf
            - "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
            - --conf
            - "spark.hadoop.fs.s3a.connection.ssl.enabled=false"
            - --conf
            - "spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
            - /app/etl_bronze_to_silver.py
            env:
            - name: MINIO_ENDPOINT
              value: "minio-service:9000"
            - name: MINIO_ACCESS_KEY
              value: "admin"
            - name: MINIO_SECRET_KEY
              value: "password123"
            volumeMounts:
            - name: spark-app
              mountPath: /app
            - name: spark-config
              mountPath: /tmp/spark-config
            resources:
              requests:
                memory: "3Gi"
                cpu: "1000m"
              limits:
                memory: "6Gi"
                cpu: "2000m"
          volumes:
          - name: spark-app
            configMap:
              name: spark-etl-code
          - name: spark-requirements
            configMap:
              name: spark-etl-requirements
          - name: spark-config
            configMap:
              name: spark-config
          restartPolicy: OnFailure
      backoffLimit: 3

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-etl-requirements
  namespace: economic-observatory
data:
  requirements.txt: |
    pyspark==3.5.0
    delta-spark==3.0.0
    pandas==2.1.4
    numpy==1.24.3
    pyarrow==14.0.1
    boto3==1.34.0
    s3fs==2023.12.2
    python-dotenv==1.0.0
    pyyaml==6.0.1
    structlog==23.2.0

---
apiVersion: v1
kind: Service
metadata:
  name: spark-streaming-service
  namespace: economic-observatory
spec:
  selector:
    app: spark-streaming
    component: consumer
  ports:
  - name: spark-ui
    port: 4040
    targetPort: 4040
  - name: metrics
    port: 8080
    targetPort: 8080
  type: ClusterIP