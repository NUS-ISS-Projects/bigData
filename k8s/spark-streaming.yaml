apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-config
  namespace: economic-observatory
data:
  spark-defaults.conf: |
    spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
    spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
    spark.serializer=org.apache.spark.serializer.KryoSerializer
    spark.sql.adaptive.enabled=true
    spark.sql.adaptive.coalescePartitions.enabled=true
    spark.sql.adaptive.skewJoin.enabled=true
    spark.sql.streaming.checkpointLocation=/tmp/spark-checkpoints
    spark.hadoop.fs.s3a.endpoint=http://minio-service:9000
    spark.hadoop.fs.s3a.path.style.access=true
    spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
    spark.hadoop.fs.s3a.connection.ssl.enabled=false

---
apiVersion: v1
kind: Secret
metadata:
  name: spark-secrets
  namespace: economic-observatory
type: Opaque
data:
  minio-access-key: YWRtaW4=  # admin base64 encoded
  minio-secret-key: cGFzc3dvcmQxMjM=  # password123 base64 encoded

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-streaming-consumer
  namespace: economic-observatory
  labels:
    app: spark-streaming
    component: consumer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-streaming
      component: consumer
  template:
    metadata:
      labels:
        app: spark-streaming
        component: consumer
    spec:
      containers:
      - name: spark-streaming
        image: bitnami/spark:3.5.0
        command: ["/bin/bash"]
        args:
        - -c
        - |
          cp /tmp/spark-config/spark-defaults.conf /opt/bitnami/spark/conf/
          export SPARK_LOCAL_IP=127.0.0.1
          export PYSPARK_SUBMIT_ARGS="--master local[1] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell"
          pip install -r /app/requirements.txt
          /opt/bitnami/spark/bin/spark-submit --master local[1] --conf spark.jars.ivy=/tmp/.ivy --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 /app/spark_streaming_consumer.py
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka-service:9092"
        - name: MINIO_ENDPOINT
          value: "minio-service:9000"
        - name: MINIO_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: spark-secrets
              key: minio-access-key
        - name: MINIO_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: spark-secrets
              key: minio-secret-key
        - name: SPARK_CONF_DIR
          value: "/opt/bitnami/spark/conf"
        - name: JAVA_HOME
          value: "/opt/bitnami/java"
        - name: SPARK_HOME
          value: "/opt/bitnami/spark"
        - name: PYSPARK_PYTHON
          value: "python3"
        - name: PYSPARK_DRIVER_PYTHON
          value: "python3"
        - name: SPARK_LOCAL_IP
          value: "127.0.0.1"
        - name: SPARK_DRIVER_MEMORY
          value: "2g"
        - name: SPARK_EXECUTOR_MEMORY
          value: "2g"
        - name: SPARK_DRIVER_OPTS
          value: "-Djava.net.preferIPv4Stack=true -Dfile.encoding=UTF-8"
        volumeMounts:
        - name: spark-app
          mountPath: /app
        - name: spark-config
          mountPath: /tmp/spark-config
        - name: checkpoint-storage
          mountPath: /tmp/spark-checkpoints
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - "ps aux | grep spark_streaming_consumer.py | grep -v grep"
          initialDelaySeconds: 600
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
      volumes:
      - name: spark-app
        configMap:
          name: spark-app-code
      - name: spark-config
        configMap:
          name: spark-config
      - name: checkpoint-storage
        emptyDir: {}
      restartPolicy: Always

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-app-code
  namespace: economic-observatory
data:
  requirements.txt: |
    pyspark==3.5.0
    delta-spark==3.0.0
    kafka-python==2.0.2
    pandas==2.1.4
    numpy==1.24.3
    pyarrow==14.0.1
    boto3==1.34.0
    s3fs==2023.12.2
    python-dotenv==1.0.0
    pyyaml==6.0.1
    structlog==23.2.0
  spark_streaming_consumer.py: |
    #!/usr/bin/env python3
    """
    Spark Structured Streaming Consumer for Economic Intelligence Platform
    Consumes data from Kafka topics and writes to Delta Lake Bronze layer
    """

    import os
    import json
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import *
    from pyspark.sql.types import *
    from delta import *
    import logging

    # Configure logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    class SparkStreamingConsumer:
        """Spark Structured Streaming consumer for Kafka to Delta Lake"""
        
        def __init__(self):
            self.spark = self._create_spark_session()
            self.kafka_bootstrap_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092')
            self.minio_endpoint = os.getenv('MINIO_ENDPOINT', 'localhost:9000')
            self.minio_access_key = os.getenv('MINIO_ACCESS_KEY', 'minioadmin')
            self.minio_secret_key = os.getenv('MINIO_SECRET_KEY', 'minioadmin')
            self.delta_path = f"s3a://bronze/"
            
            # Configure S3/MinIO settings
            self._configure_s3_settings()
            
            logger.info("Spark Streaming Consumer initialized")
        
        def _create_spark_session(self):
            """Create Spark session with Delta Lake support"""
            builder = SparkSession.builder \
                .appName("EconomicIntelligence-StreamingConsumer") \
                .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
                .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
                .config("spark.sql.streaming.checkpointLocation", "/tmp/spark-checkpoints") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
            
            return configure_spark_with_delta_pip(builder).getOrCreate()
        
        def _configure_s3_settings(self):
            """Configure Spark for S3/MinIO access"""
            hadoop_conf = self.spark.sparkContext._jsc.hadoopConfiguration()
            hadoop_conf.set("fs.s3a.endpoint", f"http://{self.minio_endpoint}")
            hadoop_conf.set("fs.s3a.access.key", self.minio_access_key)
            hadoop_conf.set("fs.s3a.secret.key", self.minio_secret_key)
            hadoop_conf.set("fs.s3a.path.style.access", "true")
            hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            hadoop_conf.set("fs.s3a.connection.ssl.enabled", "false")
        
        def create_kafka_stream(self, topic: str):
            """Create Kafka streaming DataFrame"""
            return self.spark \
                .readStream \
                .format("kafka") \
                .option("kafka.bootstrap.servers", self.kafka_bootstrap_servers) \
                .option("subscribe", topic) \
                .option("startingOffsets", "latest") \
                .option("failOnDataLoss", "false") \
                .load()
        
        def process_acra_stream(self):
            """Process ACRA companies data stream"""
            logger.info("Starting ACRA stream processing...")
            
            # Define schema for ACRA data
            acra_schema = StructType([
                StructField("uen", StringType(), True),
                StructField("reg_street_name", StringType(), True),
                StructField("entity_name", StringType(), True),
                StructField("entity_type", StringType(), True),
                StructField("entity_status", StringType(), True),
                StructField("uen_issue_date", StringType(), True),
                StructField("reg_postal_code", StringType(), True),
                StructField("source", StringType(), True),
                StructField("ingestion_timestamp", StringType(), True)
            ])
            
            # Create stream
            kafka_stream = self.create_kafka_stream("acra-companies")
            
            # Parse JSON and add metadata
            parsed_stream = kafka_stream.select(
                from_json(col("value").cast("string"), acra_schema).alias("data"),
                col("timestamp").alias("kafka_timestamp"),
                col("partition"),
                col("offset")
            ).select(
                col("data.*"),
                col("kafka_timestamp"),
                col("partition"),
                col("offset"),
                current_timestamp().alias("bronze_ingestion_timestamp")
            )
            
            # Write to Delta Lake Bronze layer
            query = parsed_stream.writeStream \
                .format("delta") \
                .outputMode("append") \
                .option("checkpointLocation", "/tmp/spark-checkpoints/acra-bronze") \
                .option("path", f"{self.delta_path}acra_companies") \
                .trigger(processingTime="30 seconds") \
                .start()
            
            return query
        
        def process_singstat_stream(self):
            """Process SingStat economics data stream"""
            logger.info("Starting SingStat stream processing...")
            
            # Define schema for SingStat data
            singstat_schema = StructType([
                StructField("table_id", StringType(), True),
                StructField("series_id", StringType(), True),
                StructField("data_type", StringType(), True),
                StructField("period", StringType(), True),
                StructField("value", StringType(), True),
                StructField("unit", StringType(), True),
                StructField("source", StringType(), True),
                StructField("ingestion_timestamp", StringType(), True)
            ])
            
            kafka_stream = self.create_kafka_stream("singstat-economics")
            
            parsed_stream = kafka_stream.select(
                from_json(col("value").cast("string"), singstat_schema).alias("data"),
                col("timestamp").alias("kafka_timestamp"),
                col("partition"),
                col("offset")
            ).select(
                col("data.*"),
                col("kafka_timestamp"),
                col("partition"),
                col("offset"),
                current_timestamp().alias("bronze_ingestion_timestamp")
            )
            
            query = parsed_stream.writeStream \
                .format("delta") \
                .outputMode("append") \
                .option("checkpointLocation", "/tmp/spark-checkpoints/singstat-bronze") \
                .option("path", f"{self.delta_path}singstat_economics") \
                .trigger(processingTime="30 seconds") \
                .start()
            
            return query
        
        def process_ura_stream(self):
            """Process URA geospatial data stream"""
            logger.info("Starting URA stream processing...")
            
            # Define schema for URA data
            ura_schema = StructType([
                StructField("project", StringType(), True),
                StructField("street", StringType(), True),
                StructField("x", StringType(), True),
                StructField("y", StringType(), True),
                StructField("lease_commence_date", StringType(), True),
                StructField("property_type", StringType(), True),
                StructField("district", StringType(), True),
                StructField("tenure", StringType(), True),
                StructField("built_year", StringType(), True),
                StructField("source", StringType(), True),
                StructField("ingestion_timestamp", StringType(), True)
            ])
            
            kafka_stream = self.create_kafka_stream("ura-geospatial")
            
            parsed_stream = kafka_stream.select(
                from_json(col("value").cast("string"), ura_schema).alias("data"),
                col("timestamp").alias("kafka_timestamp"),
                col("partition"),
                col("offset")
            ).select(
                col("data.*"),
                col("kafka_timestamp"),
                col("partition"),
                col("offset"),
                current_timestamp().alias("bronze_ingestion_timestamp")
            )
            
            query = parsed_stream.writeStream \
                .format("delta") \
                .outputMode("append") \
                .option("checkpointLocation", "/tmp/spark-checkpoints/ura-bronze") \
                .option("path", f"{self.delta_path}ura_geospatial") \
                .trigger(processingTime="30 seconds") \
                .start()
            
            return query
        
        def start_all_streams(self):
            """Start all streaming queries"""
            logger.info("Starting all Kafka to Delta Lake streams...")
            
            queries = [
                self.process_acra_stream(),
                self.process_singstat_stream(),
                self.process_ura_stream()
            ]
            
            logger.info(f"Started {len(queries)} streaming queries")
            
            # Wait for all queries to terminate
            try:
                for query in queries:
                    query.awaitTermination()
            except KeyboardInterrupt:
                logger.info("Stopping all streams...")
                for query in queries:
                    query.stop()
            
            self.spark.stop()
            logger.info("All streams stopped")

    def main():
        consumer = SparkStreamingConsumer()
        consumer.start_all_streams()

    if __name__ == "__main__":
        main()

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: spark-etl-bronze-to-silver
  namespace: economic-observatory
spec:
  schedule: "0 */6 * * *"  # Run every 6 hours
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: spark-etl
            image: bitnami/spark:3.5.0
            command: ["/bin/bash"]
            args:
            - -c
            - |
              pip install -r /app/requirements.txt
              python /app/etl_bronze_to_silver.py
            env:
            - name: MINIO_ENDPOINT
              value: "minio-service:9000"
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: spark-secrets
                  key: minio-access-key
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: spark-secrets
                  key: minio-secret-key
            volumeMounts:
            - name: spark-app
              mountPath: /app
            - name: spark-config
              mountPath: /opt/bitnami/spark/conf/spark-defaults.conf
              subPath: spark-defaults.conf
            resources:
              requests:
                memory: "4Gi"
                cpu: "2000m"
              limits:
                memory: "8Gi"
                cpu: "4000m"
          volumes:
          - name: spark-app
            configMap:
              name: spark-etl-code
          - name: spark-config
            configMap:
              name: spark-config
          restartPolicy: OnFailure
      backoffLimit: 3

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-etl-code
  namespace: economic-observatory
data:
  requirements.txt: |
    pyspark==3.5.0
    # delta-spark==3.0.0  # Temporarily disabled
    pandas==2.1.4
    numpy==1.24.3
    pyarrow==14.0.1
    boto3==1.34.0
    s3fs==2023.12.2
    python-dotenv==1.0.0
    pyyaml==6.0.1
    structlog==23.2.0
  etl_bronze_to_silver.py: |
    #!/usr/bin/env python3
    """
    ETL Job: Bronze to Silver Layer Transformation
    Cleans, validates, and standardizes data from Bronze to Silver layer
    """
    
    import os
    import logging
    from datetime import datetime
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import *
    from pyspark.sql.types import *
    # from delta import *  # Temporarily disabled
    from pyspark.sql.window import Window
    
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    class BronzeToSilverETL:
        """ETL processor for Bronze to Silver layer transformation"""
        
        def __init__(self):
            self.spark = self._create_spark_session()
            self.minio_endpoint = os.getenv('MINIO_ENDPOINT', 'localhost:9000')
            self.minio_access_key = os.getenv('MINIO_ACCESS_KEY', 'minioadmin')
            self.minio_secret_key = os.getenv('MINIO_SECRET_KEY', 'minioadmin')
            self.bronze_path = "s3a://bronze/"
            self.silver_path = "s3a://silver/"
            
            # Configure S3/MinIO settings
            self._configure_s3_settings()
            
            logger.info("Bronze to Silver ETL initialized")
        
        def _create_spark_session(self):
            """Create Spark session with Delta Lake support"""
            builder = SparkSession.builder \
                .appName("EconomicIntelligence-BronzeToSilver") \
                # .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
                # .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .config("spark.sql.adaptive.skewJoin.enabled", "true")
            
            return builder.getOrCreate()
        
        def _configure_s3_settings(self):
            """Configure Spark for S3/MinIO access"""
            hadoop_conf = self.spark.sparkContext._jsc.hadoopConfiguration()
            hadoop_conf.set("fs.s3a.endpoint", f"http://{self.minio_endpoint}")
            hadoop_conf.set("fs.s3a.access.key", self.minio_access_key)
            hadoop_conf.set("fs.s3a.secret.key", self.minio_secret_key)
            hadoop_conf.set("fs.s3a.path.style.access", "true")
            hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            hadoop_conf.set("fs.s3a.connection.ssl.enabled", "false")
        
        def read_bronze_table(self, table_name: str):
            """Read data from Bronze layer Delta table"""
            try:
                return self.spark.read.parquet(f"{self.bronze_path}{table_name}")
            except Exception as e:
                logger.error(f"Error reading bronze table {table_name}: {e}")
                return None
        
        def write_silver_table(self, df, table_name: str, mode: str = "overwrite"):
            """Write data to Silver layer Delta table"""
            try:
                df.write \
                     .mode(mode) \
                     .parquet(f"{self.silver_path}{table_name}")
                
                logger.info(f"Successfully wrote {df.count()} records to silver.{table_name}")
            except Exception as e:
                logger.error(f"Error writing to silver table {table_name}: {e}")
                raise
        
        def run_etl(self):
            """Run complete Bronze to Silver ETL process"""
            logger.info("Starting Bronze to Silver ETL process...")
            
            try:
                # Check if bronze data exists
                logger.info("Checking for bronze data...")
                
                # For now, just create a simple test to verify the ETL can run
                test_data = self.spark.createDataFrame([
                    ("test_uen", "Test Company", "LIVE", "2024-01-01"),
                    ("test_uen2", "Another Company", "ACTIVE", "2024-01-02")
                ], ["uen", "entity_name", "entity_status", "uen_issue_date"])
                
                # Add required columns for silver layer
                processed_data = test_data \
                    .withColumn("entity_type", lit("COMPANY")) \
                    .withColumn("reg_street_name", lit("Test Street")) \
                    .withColumn("reg_postal_code", lit("123456")) \
                    .withColumn("source", lit("test")) \
                    .withColumn("ingestion_timestamp", current_timestamp()) \
                    .withColumn("bronze_ingestion_timestamp", current_timestamp()) \
                    .withColumn("silver_processed_timestamp", current_timestamp()) \
                    .withColumn("is_active", when(col("entity_status").isin("LIVE", "ACTIVE"), True).otherwise(False)) \
                    .withColumn("data_quality_score", lit(1.0))
                
                # Write test data to silver layer
                self.write_silver_table(processed_data, "test_companies_clean")
                
                logger.info("Bronze to Silver ETL completed successfully")
                
            except Exception as e:
                logger.error(f"ETL process failed: {e}")
                raise
            finally:
                self.spark.stop()
    
    def main():
        etl = BronzeToSilverETL()
        etl.run_etl()
    
    if __name__ == "__main__":
        main()

---
apiVersion: v1
kind: Service
metadata:
  name: spark-streaming-service
  namespace: economic-observatory
spec:
  selector:
    app: spark-streaming
    component: consumer
  ports:
  - name: spark-ui
    port: 4040
    targetPort: 4040
  - name: metrics
    port: 8080
    targetPort: 8080
  type: ClusterIP