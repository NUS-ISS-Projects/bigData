apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-config
  namespace: economic-observatory
data:
  spark-defaults.conf: |
    spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
    spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
    spark.serializer=org.apache.spark.serializer.KryoSerializer
    spark.sql.adaptive.enabled=true
    spark.sql.adaptive.coalescePartitions.enabled=true
    spark.sql.adaptive.skewJoin.enabled=true
    spark.sql.streaming.checkpointLocation=/tmp/spark-checkpoints

---
apiVersion: v1
kind: Secret
metadata:
  name: spark-secrets
  namespace: economic-observatory
type: Opaque
data:
  minio-access-key: YWRtaW4=  # admin base64 encoded
  minio-secret-key: cGFzc3dvcmQxMjM=  # password123 base64 encoded

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-streaming-consumer
  namespace: economic-observatory
  labels:
    app: spark-streaming
    component: consumer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-streaming
      component: consumer
  template:
    metadata:
      labels:
        app: spark-streaming
        component: consumer
    spec:
      containers:
      - name: spark-streaming
        image: economic-observatory/spark-streaming:latest
        imagePullPolicy: IfNotPresent
        command: ["/opt/bitnami/spark/bin/spark-submit"]
        args:
        - --master
        - local[1]
        - --packages
        - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-spark_2.12:3.0.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262
        - --conf
        - "spark.hadoop.fs.s3a.endpoint=http://minio-service:9000"
        - --conf
        - "spark.hadoop.fs.s3a.access.key=$(MINIO_ACCESS_KEY)"
        - --conf
        - "spark.hadoop.fs.s3a.secret.key=$(MINIO_SECRET_KEY)"
        - --conf
        - "spark.hadoop.fs.s3a.path.style.access=true"
        - --conf
        - "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
        - --conf
        - "spark.hadoop.fs.s3a.connection.ssl.enabled=false"
        - --conf
        - "spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
        - /app/spark_streaming_consumer.py
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka-service.economic-observatory.svc.cluster.local:9092"
        - name: MINIO_ENDPOINT
          value: "minio-service.economic-observatory.svc.cluster.local:9000"
        - name: MINIO_ACCESS_KEY
          value: "admin"
        - name: MINIO_SECRET_KEY
          value: "password123"
        - name: SPARK_CONF_DIR
          value: "/opt/bitnami/spark/conf"
        - name: JAVA_HOME
          value: "/opt/bitnami/java"
        - name: SPARK_HOME
          value: "/opt/bitnami/spark"
        - name: PYSPARK_PYTHON
          value: "python3"
        - name: PYSPARK_DRIVER_PYTHON
          value: "python3"
        - name: HOME
          value: "/tmp"
        - name: SPARK_SUBMIT_OPTS
          value: "-Dspark.jars.ivy=/tmp/.ivy"
        - name: SPARK_LOCAL_IP
          value: "127.0.0.1"
        - name: SPARK_DRIVER_MEMORY
          value: "1g"
        - name: SPARK_EXECUTOR_MEMORY
          value: "1g"
        - name: SPARK_DRIVER_OPTS
          value: "-Djava.net.preferIPv4Stack=true -Dfile.encoding=UTF-8"
        volumeMounts:
        - name: spark-app
          mountPath: /app
        - name: spark-config
          mountPath: /tmp/spark-config
        - name: checkpoint-storage
          mountPath: /tmp/spark-checkpoints
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - "ps aux | grep spark_streaming_consumer.py | grep -v grep"
          initialDelaySeconds: 120
          periodSeconds: 60
          timeoutSeconds: 10
          failureThreshold: 5
      volumes:
      - name: spark-app
        configMap:
          name: spark-streaming-code
      - name: spark-config
        configMap:
          name: spark-config
      - name: checkpoint-storage
        emptyDir: {}
      restartPolicy: Always

# Note: ConfigMap will be created by setup script from external files

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-app-code
  namespace: economic-observatory

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: spark-etl-bronze-to-silver
  namespace: economic-observatory
spec:
  schedule: "0 */6 * * *"  # Run every 6 hours
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: spark-etl
            image: economic-observatory/spark-streaming:latest
            imagePullPolicy: IfNotPresent
            command: ["/opt/bitnami/spark/bin/spark-submit"]
            args:
            - --master
            - local[2]
            - --packages
            - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-spark_2.12:3.0.0
            - --conf
            - "spark.hadoop.fs.s3a.endpoint=http://minio-service:9000"
            - --conf
            - "spark.hadoop.fs.s3a.access.key=admin"
            - --conf
            - "spark.hadoop.fs.s3a.secret.key=password123"
            - --conf
            - "spark.hadoop.fs.s3a.path.style.access=true"
            - --conf
            - "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
            - --conf
            - "spark.hadoop.fs.s3a.connection.ssl.enabled=false"
            - --conf
            - "spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
            - /app/etl_bronze_to_silver.py
            env:
            - name: MINIO_ENDPOINT
              value: "minio-service:9000"
            - name: MINIO_ACCESS_KEY
              value: "admin"
            - name: MINIO_SECRET_KEY
              value: "password123"
            volumeMounts:
            - name: spark-app
              mountPath: /app
            - name: spark-config
              mountPath: /tmp/spark-config
            resources:
              requests:
                memory: "2Gi"
                cpu: "500m"
              limits:
                memory: "4Gi"
                cpu: "1000m"
          volumes:
          - name: spark-app
            configMap:
              name: spark-etl-code
          - name: spark-config
            configMap:
              name: spark-config
          restartPolicy: OnFailure
      backoffLimit: 3

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-etl-code
  namespace: economic-observatory
data:
  requirements.txt: |
    pyspark==3.5.0
    delta-spark==3.0.0
    pandas==2.1.4
    numpy==1.24.3
    pyarrow==14.0.1
    boto3==1.34.0
    s3fs==2023.12.2
    python-dotenv==1.0.0
    pyyaml==6.0.1
    structlog==23.2.0
  etl_bronze_to_silver.py: |
    #!/usr/bin/env python3
    """
    ETL Job: Bronze to Silver Layer Transformation
    Cleans, validates, and standardizes data from Bronze to Silver layer
    """
    
    import os
    import logging
    from datetime import datetime
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import *
    from pyspark.sql.types import *
    from delta import *
    from pyspark.sql.window import Window
    
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    class BronzeToSilverETL:
        """ETL processor for Bronze to Silver layer transformation"""
        
        def __init__(self):
            self.spark = self._create_spark_session()
            self.minio_endpoint = os.getenv('MINIO_ENDPOINT', 'localhost:9000')
            self.minio_access_key = os.getenv('MINIO_ACCESS_KEY', 'admin')
            self.minio_secret_key = os.getenv('MINIO_SECRET_KEY', 'password123')
            self.bronze_path = "s3a://bronze/"
            self.silver_path = "s3a://silver/"
            
            # Configure S3/MinIO settings
            self._configure_s3_settings()
            
            logger.info("Bronze to Silver ETL initialized")
        
        def _create_spark_session(self):
            """Create Spark session with Delta Lake support"""
            builder = SparkSession.builder \
                .appName("EconomicIntelligence-BronzeToSilver") \
                .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
                .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .config("spark.sql.adaptive.skewJoin.enabled", "true")
            
            return configure_spark_with_delta_pip(builder).getOrCreate()
        
        def _configure_s3_settings(self):
            """Configure Spark for S3/MinIO access"""
            hadoop_conf = self.spark.sparkContext._jsc.hadoopConfiguration()
            hadoop_conf.set("fs.s3a.endpoint", f"http://{self.minio_endpoint}")
            hadoop_conf.set("fs.s3a.access.key", self.minio_access_key)
            hadoop_conf.set("fs.s3a.secret.key", self.minio_secret_key)
            hadoop_conf.set("fs.s3a.path.style.access", "true")
            hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            hadoop_conf.set("fs.s3a.connection.ssl.enabled", "false")
        
        def read_bronze_table(self, table_name: str):
            """Read data from Bronze layer Delta table"""
            try:
                return self.spark.read.format("delta").load(f"{self.bronze_path}{table_name}")
            except Exception as e:
                logger.error(f"Error reading bronze table {table_name}: {e}")
                return None
        
        def write_silver_table(self, df, table_name: str, mode: str = "overwrite"):
            """Write data to Silver layer Delta table"""
            try:
                df.write \
                     .format("delta") \
                     .mode(mode) \
                     .save(f"{self.silver_path}{table_name}")
                
                logger.info(f"Successfully wrote {df.count()} records to silver.{table_name}")
            except Exception as e:
                logger.error(f"Error writing to silver table {table_name}: {e}")
                raise
        
        def run_etl(self):
            """Run complete Bronze to Silver ETL process"""
            logger.info("Starting Bronze to Silver ETL process...")
            
            try:
                # Check if bronze data exists
                logger.info("Checking for bronze data...")
                
                # For now, just create a simple test to verify the ETL can run
                test_data = self.spark.createDataFrame([
                    ("test_uen", "Test Company", "LIVE", "2024-01-01"),
                    ("test_uen2", "Another Company", "ACTIVE", "2024-01-02")
                ], ["uen", "entity_name", "entity_status", "uen_issue_date"])
                
                # Add required columns for silver layer
                processed_data = test_data \
                    .withColumn("entity_type", lit("COMPANY")) \
                    .withColumn("reg_street_name", lit("Test Street")) \
                    .withColumn("reg_postal_code", lit("123456")) \
                    .withColumn("source", lit("test")) \
                    .withColumn("ingestion_timestamp", current_timestamp()) \
                    .withColumn("bronze_ingestion_timestamp", current_timestamp()) \
                    .withColumn("silver_processed_timestamp", current_timestamp()) \
                    .withColumn("is_active", when(col("entity_status").isin("LIVE", "ACTIVE"), True).otherwise(False)) \
                    .withColumn("data_quality_score", lit(1.0))
                
                # Write test data to silver layer
                self.write_silver_table(processed_data, "test_companies_clean")
                
                logger.info("Bronze to Silver ETL completed successfully")
                
            except Exception as e:
                logger.error(f"ETL process failed: {e}")
                raise
            finally:
                self.spark.stop()
    
    def main():
        etl = BronzeToSilverETL()
        etl.run_etl()
    
    if __name__ == "__main__":
        main()

---
apiVersion: v1
kind: Service
metadata:
  name: spark-streaming-service
  namespace: economic-observatory
spec:
  selector:
    app: spark-streaming
    component: consumer
  ports:
  - name: spark-ui
    port: 4040
    targetPort: 4040
  - name: metrics
    port: 8080
    targetPort: 8080
  type: ClusterIP